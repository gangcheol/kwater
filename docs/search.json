[
  {
    "objectID": "posts/EDA (1).html",
    "href": "posts/EDA (1).html",
    "title": "00. EDA (1)",
    "section": "",
    "text": "1 train data와 test data 분포를 살펴보고 val_data 만들기\n2 val_data를 평가 데이터로 사용해서 학습시킨 모델의 성능을 1차적으로 평가하기",
    "crumbs": [
      "Posts",
      "00. EDA (1)"
    ]
  },
  {
    "objectID": "posts/EDA (1).html#train",
    "href": "posts/EDA (1).html#train",
    "title": "00. EDA (1)",
    "section": "(1) train",
    "text": "(1) train\n\ntrain_a = pd.read_csv(\"TRAIN_A.csv\")\ntrain_b = pd.read_csv(\"TRAIN_B.csv\")\n\n1 train_a : 24/05/27 - 24/06/26 까지의 데이터 셋\n2 train_b : 24/07/01 - 24/07/29 까지의 데이터 셋\n\n학습 데이터는 30일 데이터임\n\n\ntrain_a[\"timestamp\"]\n\n0        24/05/27 00:00\n1        24/05/27 00:01\n2        24/05/27 00:02\n3        24/05/27 00:03\n4        24/05/27 00:04\n              ...      \n44096    24/06/26 14:56\n44097    24/06/26 14:57\n44098    24/06/26 14:58\n44099    24/06/26 14:59\n44100    24/06/26 15:00\nName: timestamp, Length: 44101, dtype: object\n\n\n\ntrain_b[\"timestamp\"]\n\n0         2024-07-01 0:00\n1         2024-07-01 0:01\n2         2024-07-01 0:02\n3         2024-07-01 0:03\n4         2024-07-01 0:04\n               ...       \n41755    2024-07-29 23:55\n41756    2024-07-29 23:56\n41757    2024-07-29 23:57\n41758    2024-07-29 23:58\n41759    2024-07-29 23:59\nName: timestamp, Length: 41760, dtype: object\n\n\n- 이상치 분포 확인 결과 분포가 극악이다…\n\n거의 1400:1의 분포임\n나중에 임계치 설정할 때 진짜 임계치를 높게 설정해야할 듯하다.\n\n\ntrain_a.anomaly.value_counts()\n\nanomaly\n0    44071\n1       30\nName: count, dtype: int64\n\n\n\ntrain_b.anomaly.value_counts()\n\nanomaly\n0    41727\n1       33\nName: count, dtype: int64",
    "crumbs": [
      "Posts",
      "00. EDA (1)"
    ]
  },
  {
    "objectID": "posts/EDA (1).html#test",
    "href": "posts/EDA (1).html#test",
    "title": "00. EDA (1)",
    "section": "(2) test",
    "text": "(2) test\n\ntest_c1 = pd.read_csv(\"test/C/TEST_C_0000.csv\")\ntest_c2 = pd.read_csv(\"test/C/TEST_C_2919.csv\")\n\ntest_d1 = pd.read_csv(\"test/D/TEST_D_0000.csv\")\ntest_d2 = pd.read_csv(\"test/D/TEST_D_2737.csv\")\n\n\n#test_c1\n\n\ntest_c1.shape\n\n(10080, 27)\n\n\n\ntest_c2.shape\n\n(10080, 27)\n\n\n\ntest_d1.shape\n\n(10080, 23)\n\n\n\ntest_d2.shape\n\n(9819, 23)\n\n\n1 test data는 한 개의 파일이 대략 10,000개니까 추론 시 3,000,000개를 추론해야함\n2 미친 짓임. 따라서 일단 val_data set을 생성해서 예측모델의 성능을 확보하는 게 맞는 것 같음",
    "crumbs": [
      "Posts",
      "00. EDA (1)"
    ]
  },
  {
    "objectID": "posts/01. Modeling/modeling (2).html",
    "href": "posts/01. Modeling/modeling (2).html",
    "title": "01. Anomaly Transfomer (1)",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nfrom omegaconf import DictConfig\n\nfrom types import SimpleNamespace\nfrom tqdm import tqdm\nimport os\nfrom typing import List, Dict, Union\nimport gc\nfrom torch.cuda.amp import autocast, GradScaler",
    "crumbs": [
      "Posts",
      "01. Modeling",
      "01. Anomaly Transfomer (1)"
    ]
  },
  {
    "objectID": "posts/01. Modeling/modeling (2).html#이상치근처-데이터만-추출",
    "href": "posts/01. Modeling/modeling (2).html#이상치근처-데이터만-추출",
    "title": "01. Anomaly Transfomer (1)",
    "section": "(1) 이상치근처 데이터만 추출",
    "text": "(1) 이상치근처 데이터만 추출\n\n\ntrain_a = pd.read_csv(\"TRAIN_A.csv\")[40000:].reset_index(drop=True)\ntrain_b = pd.read_csv(\"TRAIN_B.csv\")[35000:].reset_index(drop=True)\nprint(train_a.anomaly.value_counts())\nprint(train_b.anomaly.value_counts())\n\nanomaly\n0    4071\n1      30\nName: count, dtype: int64\nanomaly\n0    6727\n1      33\nName: count, dtype: int64",
    "crumbs": [
      "Posts",
      "01. Modeling",
      "01. Anomaly Transfomer (1)"
    ]
  },
  {
    "objectID": "posts/01. Modeling/modeling (2).html#파라미터-셋팅",
    "href": "posts/01. Modeling/modeling (2).html#파라미터-셋팅",
    "title": "01. Anomaly Transfomer (1)",
    "section": "(2) 파라미터 셋팅",
    "text": "(2) 파라미터 셋팅\n\nconfig1 = {\n    \"WINDOW_GIVEN\": 10,  # Extended sequence length\n    \"BATCH_SIZE\": 4,  # Reduced batch size for memory optimization\n    \"HIDDEN_DIM_LSTM\": 128,  # Increased hidden dimension\n    \"NUM_LAYERS\": 3,  # Increased number of layers\n    \"EPOCHS\": 10,  # More epochs for better training\n    \"LEARNING_RATE\": 1e-3,  # Standard learning rate\n    \"DEVICE\": rank,  # Using GPU\n    \"DROPOUT\": 0.2,  # Adjusted dropout\n}\n\nCFG = SimpleNamespace(**config1)",
    "crumbs": [
      "Posts",
      "01. Modeling",
      "01. Anomaly Transfomer (1)"
    ]
  },
  {
    "objectID": "posts/01. Modeling/modeling (2).html#data-준비",
    "href": "posts/01. Modeling/modeling (2).html#data-준비",
    "title": "01. Anomaly Transfomer (1)",
    "section": "(3) data 준비",
    "text": "(3) data 준비\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, stride: int = 1, inference: bool = False) -&gt; None:\n        \"\"\"\n        Args:\n            df: 입력 데이터프레임\n            stride: 윈도우 스트라이드\n            inference: 추론 모드 여부\n        \"\"\"\n        self.inference = inference\n        self.column_names = df.filter(regex='^P\\d+$').columns.tolist()\n        self.file_ids = df['file_id'].values if 'file_id' in df.columns else None\n\n        if inference:\n            self.values = df[self.column_names].values.astype(np.float32)\n            self._prepare_inference_data()\n        else:\n            self._prepare_training_data(df, stride)\n\n    def _normalize_columns(self, data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"벡터화된 열 정규화\"\"\"\n        # Z-Score 정규화 적용\n        means = data.mean(axis=0, keepdims=True)\n        stds = data.std(axis=0, keepdims=True)\n        return (data - means) / (stds + 1e-8)\n\n    def _prepare_inference_data(self) -&gt; None:\n        \"\"\"추론 데이터 준비 - 단일 시퀀스\"\"\"\n        self.normalized_values = self._normalize_columns(self.values)\n\n    def _prepare_training_data(self, df: pd.DataFrame, stride: int) -&gt; None:\n        \"\"\"학습 데이터 준비 - 윈도우 단위\"\"\"\n        self.values = df[self.column_names].values.astype(np.float32)\n\n        # 시작 인덱스 계산 (stride 적용)\n        potential_starts = np.arange(0, len(df) - CFG.WINDOW_GIVEN, stride)\n\n        # 각 윈도우의 이상치 비율 조건 완화\n        accident_labels = df['anomaly'].values\n        valid_starts = [\n            idx for idx in potential_starts\n            if idx + CFG.WINDOW_GIVEN &lt; len(df) and  # 범위 체크\n            np.mean(accident_labels[idx:idx + CFG.WINDOW_GIVEN]) &lt;= 0.1  # 이상치 비율 조건\n        ]\n        self.start_idx = np.array(valid_starts)\n\n        # 유효한 윈도우들만 추출하여 정규화\n        windows = np.array([\n            self.values[i:i + CFG.WINDOW_GIVEN]\n            for i in self.start_idx\n        ])\n\n        # (윈도우 수, 윈도우 크기, 특성 수)로 한번에 정규화\n        self.input_data = np.stack([\n            self._normalize_columns(window) for window in windows\n        ])\n\n    def __len__(self) -&gt; int:\n        if self.inference:\n            return len(self.column_names)\n        return len(self.start_idx) * len(self.column_names)\n\n    def __getitem__(self, idx: int) -&gt; Dict[str, Union[str, torch.Tensor]]:\n        if self.inference:\n            col_idx = idx\n            col_name = self.column_names[col_idx]\n            col_data = self.normalized_values[:, col_idx]\n            file_id = self.file_ids[idx] if self.file_ids is not None else None\n            return {\n                \"column_name\": col_name,\n                \"input\": torch.from_numpy(col_data).unsqueeze(-1),  # (time_steps, 1)\n                \"file_id\": file_id\n            }\n\n        window_idx = idx // len(self.column_names)\n        col_idx = idx % len(self.column_names)\n\n        return {\n            \"column_name\": self.column_names[col_idx],\n            \"input\": torch.from_numpy(self.input_data[window_idx, :, col_idx]).unsqueeze(-1)\n        }\n\n\na. 시계열 데이터 생성\n\ntrain_dataset_A = TimeSeriesDataset(train_a, stride=60)\ntrain_dataset_B = TimeSeriesDataset(train_a, stride=60)\n\ntrain_dataset_A_B = torch.utils.data.ConcatDataset([train_dataset_A, train_dataset_B])\n\n\n\nb. data loader\n- 모델 학습 과정 시 각 step 마다 데이터를 batch size 크기로 분할하여 넣어 효과적이고 효율적인 학습 진행을 돕기 위해 사용\n\n데이터를 1개씩 넣으면 학습이 너무 오래 걸리고, 전체를 한 번에 넣으면 컴퓨터 자우너이 데이터를 감당할 수 없기 때문에 위 같은 방법을 사용\nDataLoader 객체는 일종의 generator 형태로, 인덱싱이 불가능하고 for문 순회 등의 방법을 통하여 분할된 데이터를 일일이 가져와야 한다.\n\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset_A_B, \n                                            batch_size=CFG.BATCH_SIZE, \n                                            shuffle=True)",
    "crumbs": [
      "Posts",
      "01. Modeling",
      "01. Anomaly Transfomer (1)"
    ]
  },
  {
    "objectID": "posts/01. Modeling/modeling (2).html#추론-및-test-데이터-전처리-함수",
    "href": "posts/01. Modeling/modeling (2).html#추론-및-test-데이터-전처리-함수",
    "title": "01. Anomaly Transfomer (1)",
    "section": "(1) 추론 및 test 데이터 전처리 함수",
    "text": "(1) 추론 및 test 데이터 전처리 함수\n\ndef inference_test_files(MODEL, batch, device='cuda'):\n    MODEL.eval()\n    with torch.no_grad():\n        inputs = batch[\"input\"].to(device)\n        with autocast():  # Mixed Precision 활성화\n            original_hidden, reconstructed_hidden = MODEL(inputs)\n        reconstruction_loss = torch.mean((original_hidden - reconstructed_hidden) ** 2, dim=1).cpu().numpy()\n    return reconstruction_loss\n\ndef test_data_processing(test_directory):\n    test_files = [f for f in os.listdir(test_directory) if f.startswith(\"TEST\") and f.endswith(\".csv\")]\n    test_datasets = []\n    all_test_data = []\n\n    for filename in tqdm(test_files, desc='Processing test files'):\n        test_file = os.path.join(test_directory, filename)\n        df = pd.read_csv(test_file)\n        df['file_id'] = filename.replace('.csv', '')\n        individual_df = df[['timestamp', 'file_id'] + df.filter(like='P').columns.tolist()]\n        individual_dataset = TimeSeriesDataset(individual_df, inference=True)\n        test_datasets.append(individual_dataset)\n        all_test_data.append(df)\n\n    combined_dataset = torch.utils.data.ConcatDataset(test_datasets)\n\n    test_loader = torch.utils.data.DataLoader(\n        combined_dataset,\n        batch_size=CFG.BATCH_SIZE,\n        shuffle=False,\n        collate_fn=custom_collate_fn\n    )\n    return test_loader",
    "crumbs": [
      "Posts",
      "01. Modeling",
      "01. Anomaly Transfomer (1)"
    ]
  },
  {
    "objectID": "posts/01. Modeling/modeling (2).html#이상치-추론함수",
    "href": "posts/01. Modeling/modeling (2).html#이상치-추론함수",
    "title": "01. Anomaly Transfomer (1)",
    "section": "(2) 이상치 추론함수",
    "text": "(2) 이상치 추론함수\n\ndef detect_anomaly(MODEL, test_loader):\n    reconstruction_errors = []\n    for batch in tqdm(test_loader):\n        reconstruction_loss = inference_test_files(MODEL, batch, CFG.DEVICE)\n        for i in range(len(reconstruction_loss)):\n            reconstruction_errors.append({\n                \"ID\": batch[\"file_id\"][i],\n                \"column_name\": batch[\"column_name\"][i],\n                \"reconstruction_error\": reconstruction_loss[i]\n            })\n\n    errors_df = pd.DataFrame(reconstruction_errors)\n\n    flag_columns = []\n    for column in sorted(errors_df['column_name'].unique()):\n        flag_column = f'{column}_flag'\n        errors_df[flag_column] = (errors_df.loc[errors_df['column_name'] == column, 'reconstruction_error'] &gt; THRESHOLD).astype(int)\n        flag_columns.append(flag_column)\n\n    errors_df_pivot = errors_df.pivot_table(index='ID', \n                                            columns='column_name', \n                                            values=flag_columns, \n                                            aggfunc='first')\n    errors_df_pivot.columns = [f'{col[1]}' for col in errors_df_pivot.columns]\n    errors_df_flat = errors_df_pivot.reset_index()\n\n    errors_df_flat['flag_list'] = errors_df_flat.loc[:, 'P1':'P' + str(len(flag_columns))].apply(lambda x: x.tolist(), axis=1).apply(lambda x: [int(i) for i in x])\n    return errors_df_flat[[\"ID\", \"flag_list\"]]\n\n\nC_DATA = test_data_processing(test_directory=\"./test/C\")\nD_DATA = test_data_processing(test_directory=\"./test/D\")\n\n#C_D_list = pd.concat([C_list, D_list])\n\nProcessing test files: 100%|██████████| 2920/2920 [01:03&lt;00:00, 46.20it/s]\nProcessing test files: 100%|██████████| 2738/2738 [00:53&lt;00:00, 51.20it/s]\n\n\n\nC_DATA\n\n&lt;torch.utils.data.dataloader.DataLoader at 0x7fe3305d42e0&gt;\n\n\n\nC_list = detect_anomaly(INFER_MODEL, C_DATA)\n#D_list = detect_anomaly(INFER_MODEL, test_directory=\"./D\")\n\n  0%|          | 0/5840 [00:00&lt;?, ?it/s]/tmp/ipykernel_2764838/3225321808.py:5: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():  # Mixed Precision 활성화\n  8%|▊         | 452/5840 [00:54&lt;10:53,  8.24it/s]\n\n\nKeyboardInterrupt: \n\n\n\nsample_submission = pd.read_csv(\"./sample_submission.csv\")\n# 매핑된 값으로 업데이트하되, 매핑되지 않은 경우 기존 값 유지\nflag_mapping = C_D_list.set_index(\"ID\")[\"flag_list\"]\nsample_submission[\"flag_list\"] = sample_submission[\"ID\"].map(flag_mapping).fillna(sample_submission[\"flag_list\"])\n\nsample_submission.to_csv(\"./baseline_submission.csv\", index=False)",
    "crumbs": [
      "Posts",
      "01. Modeling",
      "01. Anomaly Transfomer (1)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "수로뻥~!",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 14, 2024\n\n\n01. Anomaly Transfomer (1)\n\n\nGC \n\n\n\n\nDec 12, 2024\n\n\n00. EDA (1)\n\n\nGC \n\n\n\n\nDec 11, 2024\n\n\n00. Anomaly Transfomer Basic\n\n\nGC \n\n\n\n\nAug 15, 2022\n\n\n00. Quarto Guide\n\n\nGC \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01. Modeling/modeiling (1).html",
    "href": "posts/01. Modeling/modeiling (1).html",
    "title": "00. Anomaly Transfomer Basic",
    "section": "",
    "text": "#!pip install torch torchvision numpy matplotlib\n\n\nimport\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n1. Anomaly Transformer 모델 정의\n\nclass AnomalyTransformer(nn.Module):\n    def __init__(self, input_dim, seq_len, num_heads, num_layers, dropout=0.1):\n        super(AnomalyTransformer, self).__init__()\n        self.input_dim = input_dim\n        self.seq_len = seq_len\n        self.num_heads = num_heads\n\n        # Embedding Layer\n        self.embedding = nn.Linear(input_dim, input_dim)\n\n        # Positional Encoding\n        self.positional_encoding = nn.Parameter(torch.randn(1, seq_len, input_dim))\n\n        # Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dropout=dropout)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # Attention Score Calculation\n        self.attention_fc = nn.Linear(input_dim, seq_len)  # 이 부분의 출력 크기를 확인\n\n        # Discrepancy Calculation\n        self.discrepancy_fc = nn.Linear(seq_len * seq_len, 1)  # attention_map의 펼친 차원을 입력\n\n    def forward(self, x):\n        \"\"\"\n        Input shape: (batch_size, seq_len, input_dim)\n        Output: (batch_size, 1) - Anomaly scores\n        \"\"\"\n        batch_size, seq_len, _ = x.size()\n        assert seq_len == self.seq_len, f\"Expected seq_len={self.seq_len}, but got seq_len={seq_len}\"\n\n        # Embedding + Positional Encoding\n        x = self.embedding(x) + self.positional_encoding\n\n        # Transformer Encoder\n        x = self.transformer_encoder(x)\n\n        # Attention Score Calculation\n        attention_map = torch.matmul(x, x.transpose(1, 2)) / self.num_heads  # (batch_size, seq_len, seq_len)\n\n        # Flatten the attention map for fully connected layer\n        attention_flat = attention_map.view(batch_size, -1)  # (batch_size, seq_len * seq_len)\n        discrepancy = self.discrepancy_fc(attention_flat)  # (batch_size, 1)\n\n        return discrepancy\n\n\n\n2. 데이터 준비\n\nimport numpy as np\nimport torch\n\n# Dummy 데이터 생성\ndef create_dummy_data(num_samples=1000, sequence_length=30, feature_dim=10):\n    data = np.random.normal(size=(num_samples, sequence_length, feature_dim))\n    labels = np.random.choice([0, 1], size=num_samples, p = [0.95, 0.05])  # 정상(0), 이상(1) 레이블\n    return data, labels\n\n# 데이터 로드\nsequence_length = 30\nfeature_dim = 12\nnum_samples = 1000\n\ndata, labels = create_dummy_data(num_samples, sequence_length, feature_dim)\n\n# PyTorch 텐서로 변환\ndata_tensor = torch.tensor(data, dtype=torch.float32)\nlabels_tensor = torch.tensor(labels, dtype=torch.float32)\n\n\nlabels_tensor.unique(return_counts = True)\n\n(tensor([0., 1.]), tensor([956,  44]))\n\n\n\n# 데이터 크기 확인\nprint(f\"Input shape before: {data_tensor.shape}\")\n\n# 필요 시 크기 조정\nif len(data_tensor.shape) == 2:\n    data_tensor = data_tensor.unsqueeze(-1)  # (batch_size, seq_len, input_dim)\n\nprint(f\"Input shape after adjustment: {data_tensor.shape}\")\n\nInput shape before: torch.Size([1000, 30, 12])\nInput shape after adjustment: torch.Size([1000, 30, 12])\n\n\n\n\n3. 모델 학습\n\n# 모델 초기화\ninput_dim = feature_dim\nsequence_length = 30  # seq_len\nnum_heads = 2\nnum_layers = 3\ndropout = 0.1\n\nmodel = AnomalyTransformer(input_dim=input_dim, seq_len=sequence_length, num_heads=num_heads, num_layers=num_layers, dropout=dropout)\n\n# 손실 함수 및 옵티마이저\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# 데이터 준비\ntrain_data = data_tensor[:800]\ntrain_labels = labels_tensor[:800]\n\n# 학습 루프\nepochs = 10\nbatch_size = 32\n\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss = 0.0\n    for i in range(0, len(train_data), batch_size):\n        batch_data = train_data[i:i+batch_size]\n        optimizer.zero_grad()\n        scores = model(batch_data)\n        loss = criterion(scores, torch.zeros_like(scores))  # 정상 데이터를 기반으로 학습\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss / len(train_data)}\")\n\n/home/jupyter-rkdcjf8232/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n\n\nEpoch 1/10, Loss: 0.07684405563399195\nEpoch 2/10, Loss: 0.004637479749508202\nEpoch 3/10, Loss: 0.001985558783635497\nEpoch 4/10, Loss: 0.0015098400111310183\nEpoch 5/10, Loss: 0.0009821641095913948\nEpoch 6/10, Loss: 0.0008279510936699808\nEpoch 7/10, Loss: 0.0008808718901127577\nEpoch 8/10, Loss: 0.0007669400749728084\nEpoch 9/10, Loss: 0.0008671140717342496\nEpoch 10/10, Loss: 0.0007926417014095932\n\n\n\n\n4. 결과 시각화 및 이상 점수 계산\n\n# 평가 데이터\ntest_data = data_tensor[800:]\n\n# 이상 점수 계산\ndef calculate_anomaly_scores(data, model):\n    model.eval()\n    with torch.no_grad():\n        scores = model(data)\n    return scores\n\n# 점수 계산\nanomaly_scores = calculate_anomaly_scores(test_data, model).cpu().numpy()\n\n# 임계값 설정\nthreshold = np.percentile(anomaly_scores, 95)\n\n# 이상 여부 판정\npredicted_anomalies = anomaly_scores &gt; threshold\n\n# 시각화\nimport matplotlib.pyplot as plt\n\nplt.hist(anomaly_scores, bins=50, alpha=0.7, label=\"Anomaly Scores\")\nplt.axvline(threshold, color='r', linestyle='--', label=\"Threshold\")\nplt.legend()\nplt.title(\"Anomaly Scores Distribution\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\ndo next\n1 이제 이거를 우리 데이터를 끌고와서 적용시키기\n2 제출함수 작성 고민…",
    "crumbs": [
      "Posts",
      "01. Modeling",
      "00. Anomaly Transfomer Basic"
    ]
  },
  {
    "objectID": "posts/2023-07-31-00. Quarto Guide.html",
    "href": "posts/2023-07-31-00. Quarto Guide.html",
    "title": "00. Quarto Guide",
    "section": "",
    "text": "- 어… 일단 평소에도 quarto를 이용해서 웹사이트를 관리했지만… 뭔가 처음 깃허브를 접하구 하시는 분들은 이 플랫폼을 사용할 때 되게 난항이 있을것 같다… (내가 그랬다...)\n- 그리고 원래 만들어 놓았던 사이트는 뭔가 좀 지저분한 느낌이 들어서….\n- 이번 수업 멘토 학생들이 기록할 때는 본인만의 방식으로 자유롭게 기록 장소를 만들었으면 좋겠다(\\(\\star\\star\\star\\))\n- 이참에 절차를 확실히 만들어 보자!\n\nquarto 장점 1 : 마크다운 형식 지원, 코드 출력값(그래프, 코드 실행결과) 지원\n\n이를 통해서, 코드와 분석이론을 한꺼번에 정리할 수 있음,\n\nquarto 장점 2 : 만들기 쉬움, 복잡한 css코드 같은 것 몰라도 됨, github 데스크탑과 로컬 폴더 연결만하면 마우스 클릭만으로 웹사이트 제작 가능\n\n\n\n- quarto download link : 여기서 quarto를 다운받자!\n\n\n\n1 Terminal을 켠다음에 아래와 같은 명령어를 입력한다!\n(그.. 명령어 입력할 때 현재 자기 주피터 킬때 켜지는 폴더로 옮긴 다음에 수행하자… 골치 아프다ㅜㅜ)\nquarto create project website gcsite\n2 그러면 다음과 같은 이미지가 보인다\n\n3 저기 open with 어찌고 보이는데 d버튼 누르면 (don’t open)으로 넘어가니 그걸 선택한 후 엔터를 눌러준다!\n4 그러면 아래 이미지처럼 맨 밑에 gcsite라는 폴더가 생긴 것을 볼 수 있다.\n\n\n\n\n1 깃허브 데스크탑을 킨다.\n2 상단 메뉴바 \\(\\to\\) File \\(\\to\\) Add local repository\n3 그러면 아래와 같은 경고문이 뜬다.\n\n5 local하고 연결하고 싶은데 깃허브에는 gcsite가 없으니 만들어 달라는 것임. “create a repository” 를 눌러주자.\n\n6 create repository를 선택한다.\n7 그러면 깃허브 데스크탑에서 너 방금 만든거 너꺼 깃허브에 Publish 할거냐고 물어봄\n\n8 Publish repository 눌러주면 끝~~ (단, publish할 때 private 체크박스는 해제하구 하자!)\n9 그 다음 내가 생성산 gcsite 저장소 setting으로 넘어가서 pages를 클릭!\n10 아래와 같이 branch를 수정 후 save 버튼 눌러주자\n\n\n\n\n1 quarto 원리 : 작성한 ipynb파일 html파일로 출력해서 그 html파일들로 웹사이트를 구성하는 것1\n2 step1. posts와 docs라는 폴더를 만들자\n\nposts는 내가 작성하는 ipynb파일들이 들어갈거고, docs에는 html파일이 들어갈 것이다.\n\n3 step2. index 파일 수정\n\nindex파일은 뭐랄까 네비게이터 역할이랄까 아래와 같이 바꿔주자\n\n---\ntitle: \"GC site\"\nlisting:\n  contents: posts\n  sort: [date desc, title]\n  type: table\n  categories: true\n  sort-ui: true\n  filter-ui: true\npage-layout: full\ntitle-block-banner: true\n---\n4 step3. _quarto.yml 파일 수정 \\(\\to\\) 템플릿이랑 디자인 이쁜거 많으니 본인 입맛에 맞게 수정하면 됩니당\nproject:\n  type: website\n  output-dir : docs  \nwebsite:\n  title: \"GC site\"\n  page-navigation: true\n  navbar:\n    right:\n      - icon : github\n        href : https://github.com/gangcheol/\n  sidebar:\n    style: \"docked\"\n    search: True\n    contents: auto\n    \nformat:\n  html:\n    css: styles.css\n    toc: true\n    code-fold : False\n    code-line-numbers : True\n    code-copy : True\n\ntheme :\n  light : flatly\n  \neditor : visual\n5 step4. 앞서 만든 posts폴더에 아무 파일이나 만들어보자\n\n6 step5. 그 후 다시 터미널에서 내가 생성한 폴더로 이동\n필자의 경우는 cd gcsite\n7 step6. quarto render 입력\n8 step7. github desktop보면 난리가 났을 것이다. 막 일을 좀 많이 했음.\n\n로컬하고 연결되어 있으니 로컬이 하고 있는 걸 다적어서 그럼\n\n\n\n저기 내가 밑에 이러한 기록을 init이라고 써놨다. 저건 내가 로컬에서 한 행동을 내 깃허브 로컬에 저장할 건데, 그 행동을 init이라고 쓴거\n이제 저 Commit to main 버튼을 눌러주고 가운데 화면에 뜨는 push origin을 눌러주자!\n\n9 마지막!! 아까 깃허브 로컬 셋팅에서 pases란에 잠시 후에 들어가보면 다음과 같은 것을 볼 수 있다.\n\n- 아래 링크로 들어가면 내가 만든 웹사이트 초안을 볼 수 있다.\n- 링크",
    "crumbs": [
      "Posts",
      "00. Quarto Guide"
    ]
  },
  {
    "objectID": "posts/2023-07-31-00. Quarto Guide.html#install",
    "href": "posts/2023-07-31-00. Quarto Guide.html#install",
    "title": "00. Quarto Guide",
    "section": "",
    "text": "- quarto download link : 여기서 quarto를 다운받자!",
    "crumbs": [
      "Posts",
      "00. Quarto Guide"
    ]
  },
  {
    "objectID": "posts/2023-07-31-00. Quarto Guide.html#website-생성",
    "href": "posts/2023-07-31-00. Quarto Guide.html#website-생성",
    "title": "00. Quarto Guide",
    "section": "",
    "text": "1 Terminal을 켠다음에 아래와 같은 명령어를 입력한다!\n(그.. 명령어 입력할 때 현재 자기 주피터 킬때 켜지는 폴더로 옮긴 다음에 수행하자… 골치 아프다ㅜㅜ)\nquarto create project website gcsite\n2 그러면 다음과 같은 이미지가 보인다\n\n3 저기 open with 어찌고 보이는데 d버튼 누르면 (don’t open)으로 넘어가니 그걸 선택한 후 엔터를 눌러준다!\n4 그러면 아래 이미지처럼 맨 밑에 gcsite라는 폴더가 생긴 것을 볼 수 있다.",
    "crumbs": [
      "Posts",
      "00. Quarto Guide"
    ]
  },
  {
    "objectID": "posts/2023-07-31-00. Quarto Guide.html#깃허브-로컬-연결",
    "href": "posts/2023-07-31-00. Quarto Guide.html#깃허브-로컬-연결",
    "title": "00. Quarto Guide",
    "section": "",
    "text": "1 깃허브 데스크탑을 킨다.\n2 상단 메뉴바 \\(\\to\\) File \\(\\to\\) Add local repository\n3 그러면 아래와 같은 경고문이 뜬다.\n\n5 local하고 연결하고 싶은데 깃허브에는 gcsite가 없으니 만들어 달라는 것임. “create a repository” 를 눌러주자.\n\n6 create repository를 선택한다.\n7 그러면 깃허브 데스크탑에서 너 방금 만든거 너꺼 깃허브에 Publish 할거냐고 물어봄\n\n8 Publish repository 눌러주면 끝~~ (단, publish할 때 private 체크박스는 해제하구 하자!)\n9 그 다음 내가 생성산 gcsite 저장소 setting으로 넘어가서 pages를 클릭!\n10 아래와 같이 branch를 수정 후 save 버튼 눌러주자",
    "crumbs": [
      "Posts",
      "00. Quarto Guide"
    ]
  },
  {
    "objectID": "posts/2023-07-31-00. Quarto Guide.html#문서-생성",
    "href": "posts/2023-07-31-00. Quarto Guide.html#문서-생성",
    "title": "00. Quarto Guide",
    "section": "",
    "text": "1 quarto 원리 : 작성한 ipynb파일 html파일로 출력해서 그 html파일들로 웹사이트를 구성하는 것1\n2 step1. posts와 docs라는 폴더를 만들자\n\nposts는 내가 작성하는 ipynb파일들이 들어갈거고, docs에는 html파일이 들어갈 것이다.\n\n3 step2. index 파일 수정\n\nindex파일은 뭐랄까 네비게이터 역할이랄까 아래와 같이 바꿔주자\n\n---\ntitle: \"GC site\"\nlisting:\n  contents: posts\n  sort: [date desc, title]\n  type: table\n  categories: true\n  sort-ui: true\n  filter-ui: true\npage-layout: full\ntitle-block-banner: true\n---\n4 step3. _quarto.yml 파일 수정 \\(\\to\\) 템플릿이랑 디자인 이쁜거 많으니 본인 입맛에 맞게 수정하면 됩니당\nproject:\n  type: website\n  output-dir : docs  \nwebsite:\n  title: \"GC site\"\n  page-navigation: true\n  navbar:\n    right:\n      - icon : github\n        href : https://github.com/gangcheol/\n  sidebar:\n    style: \"docked\"\n    search: True\n    contents: auto\n    \nformat:\n  html:\n    css: styles.css\n    toc: true\n    code-fold : False\n    code-line-numbers : True\n    code-copy : True\n\ntheme :\n  light : flatly\n  \neditor : visual\n5 step4. 앞서 만든 posts폴더에 아무 파일이나 만들어보자\n\n6 step5. 그 후 다시 터미널에서 내가 생성한 폴더로 이동\n필자의 경우는 cd gcsite\n7 step6. quarto render 입력\n8 step7. github desktop보면 난리가 났을 것이다. 막 일을 좀 많이 했음.\n\n로컬하고 연결되어 있으니 로컬이 하고 있는 걸 다적어서 그럼\n\n\n\n저기 내가 밑에 이러한 기록을 init이라고 써놨다. 저건 내가 로컬에서 한 행동을 내 깃허브 로컬에 저장할 건데, 그 행동을 init이라고 쓴거\n이제 저 Commit to main 버튼을 눌러주고 가운데 화면에 뜨는 push origin을 눌러주자!\n\n9 마지막!! 아까 깃허브 로컬 셋팅에서 pases란에 잠시 후에 들어가보면 다음과 같은 것을 볼 수 있다.\n\n- 아래 링크로 들어가면 내가 만든 웹사이트 초안을 볼 수 있다.\n- 링크",
    "crumbs": [
      "Posts",
      "00. Quarto Guide"
    ]
  }
]