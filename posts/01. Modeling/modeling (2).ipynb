{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1b104b4f-03b4-4c64-8638-f8024428fb58",
   "metadata": {},
   "source": [
    "---\n",
    "title : \"01. Anomaly Transfomer (1)\"\n",
    "author : \"GC\"\n",
    "date : \"12/14/24\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f2a50e-7ff2-4db9-bf73-68eca27b388e",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "619d7f0b-4419-43bf-a74d-d77ece25ac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |code-fold : true\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import List, Dict, Union\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627cde8-be76-4069-9038-761e9bcc5c6a",
   "metadata": {},
   "source": [
    "# GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5d93a857-6af5-41f6-8142-a510aba178d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a722d969-ca41-4043-a186-5b172f6cd7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 13 19:17:55 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:1D:00.0 Off |                  N/A |\n",
      "| 17%   33C    P8             12W /  250W |    1294MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   60C    P0             69W /  250W |     932MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:1F:00.0 Off |                  N/A |\n",
      "| 26%   59C    P0             65W /  250W |     468MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:20:00.0 Off |                  N/A |\n",
      "| 25%   58C    P0             64W /  250W |     914MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:21:00.0 Off |                  N/A |\n",
      "| 23%   56C    P0             63W /  250W |   10254MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:22:00.0 Off |                  N/A |\n",
      "| 18%   34C    P8             16W /  250W |       4MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:23:00.0 Off |                  N/A |\n",
      "| 17%   37C    P8             16W /  250W |    3620MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:24:00.0 Off |                  N/A |\n",
      "| 17%   39C    P0             75W /  250W |     218MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "c8f6ecb6-76bb-467f-a408-e6fcbd4ec651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 설정\n",
    "GPU_NUM = 5\n",
    "rank = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc37d0-0552-4500-a49a-cbb4d9a12bec",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "3d1da4ee-4737-4972-922b-14874852697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly\n",
      "0    44071\n",
      "1       30\n",
      "Name: count, dtype: int64\n",
      "anomaly\n",
      "0    41727\n",
      "1       33\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_a = pd.read_csv(\"TRAIN_A.csv\")\n",
    "train_b = pd.read_csv(\"TRAIN_B.csv\")\n",
    "\n",
    "print(train_a.anomaly.value_counts())\n",
    "print(train_b.anomaly.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "d34f679b-efaf-4524-92da-c4cb99b1b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_A.loc[train_A.anomaly == 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "63fb074b-84ae-4c1e-8982-3b0c07db5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_B.loc[train_B.anomaly == 1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d067549-183a-4f74-bc14-405192c3096d",
   "metadata": {},
   "source": [
    "## (1) 이상치근처 데이터만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2489c144-f4c0-4bda-8202-f2151415ab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly\n",
      "0    4071\n",
      "1      30\n",
      "Name: count, dtype: int64\n",
      "anomaly\n",
      "0    6727\n",
      "1      33\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_a = pd.read_csv(\"TRAIN_A.csv\")[40000:].reset_index(drop=True)\n",
    "train_b = pd.read_csv(\"TRAIN_B.csv\")[35000:].reset_index(drop=True)\n",
    "print(train_a.anomaly.value_counts())\n",
    "print(train_b.anomaly.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a28ef-c1c6-46e5-9e59-509128046ecd",
   "metadata": {},
   "source": [
    "## (2) 파라미터 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "dd6b73d7-2b30-4483-a6ef-51e8d558d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\n",
    "    \"WINDOW_GIVEN\": 10,  # Extended sequence length\n",
    "    \"BATCH_SIZE\": 4,  # Reduced batch size for memory optimization\n",
    "    \"HIDDEN_DIM_LSTM\": 128,  # Increased hidden dimension\n",
    "    \"NUM_LAYERS\": 3,  # Increased number of layers\n",
    "    \"EPOCHS\": 10,  # More epochs for better training\n",
    "    \"LEARNING_RATE\": 1e-3,  # Standard learning rate\n",
    "    \"DEVICE\": rank,  # Using GPU\n",
    "    \"DROPOUT\": 0.2,  # Adjusted dropout\n",
    "}\n",
    "\n",
    "CFG = SimpleNamespace(**config1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c316c-bdce-4579-ab5f-dce61e6b091a",
   "metadata": {},
   "source": [
    "## (3) data 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3a02353c-8056-47b4-a760-f6721ad00041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | codd-fold : true\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, stride: int = 1, inference: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: 입력 데이터프레임\n",
    "            stride: 윈도우 스트라이드\n",
    "            inference: 추론 모드 여부\n",
    "        \"\"\"\n",
    "        self.inference = inference\n",
    "        self.column_names = df.filter(regex='^P\\d+$').columns.tolist()\n",
    "        self.file_ids = df['file_id'].values if 'file_id' in df.columns else None\n",
    "\n",
    "        if inference:\n",
    "            self.values = df[self.column_names].values.astype(np.float32)\n",
    "            self._prepare_inference_data()\n",
    "        else:\n",
    "            self._prepare_training_data(df, stride)\n",
    "\n",
    "    def _normalize_columns(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"벡터화된 열 정규화\"\"\"\n",
    "        # Z-Score 정규화 적용\n",
    "        means = data.mean(axis=0, keepdims=True)\n",
    "        stds = data.std(axis=0, keepdims=True)\n",
    "        return (data - means) / (stds + 1e-8)\n",
    "\n",
    "    def _prepare_inference_data(self) -> None:\n",
    "        \"\"\"추론 데이터 준비 - 단일 시퀀스\"\"\"\n",
    "        self.normalized_values = self._normalize_columns(self.values)\n",
    "\n",
    "    def _prepare_training_data(self, df: pd.DataFrame, stride: int) -> None:\n",
    "        \"\"\"학습 데이터 준비 - 윈도우 단위\"\"\"\n",
    "        self.values = df[self.column_names].values.astype(np.float32)\n",
    "\n",
    "        # 시작 인덱스 계산 (stride 적용)\n",
    "        potential_starts = np.arange(0, len(df) - CFG.WINDOW_GIVEN, stride)\n",
    "\n",
    "        # 각 윈도우의 이상치 비율 조건 완화\n",
    "        accident_labels = df['anomaly'].values\n",
    "        valid_starts = [\n",
    "            idx for idx in potential_starts\n",
    "            if idx + CFG.WINDOW_GIVEN < len(df) and  # 범위 체크\n",
    "            np.mean(accident_labels[idx:idx + CFG.WINDOW_GIVEN]) <= 0.1  # 이상치 비율 조건\n",
    "        ]\n",
    "        self.start_idx = np.array(valid_starts)\n",
    "\n",
    "        # 유효한 윈도우들만 추출하여 정규화\n",
    "        windows = np.array([\n",
    "            self.values[i:i + CFG.WINDOW_GIVEN]\n",
    "            for i in self.start_idx\n",
    "        ])\n",
    "\n",
    "        # (윈도우 수, 윈도우 크기, 특성 수)로 한번에 정규화\n",
    "        self.input_data = np.stack([\n",
    "            self._normalize_columns(window) for window in windows\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self.inference:\n",
    "            return len(self.column_names)\n",
    "        return len(self.start_idx) * len(self.column_names)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Union[str, torch.Tensor]]:\n",
    "        if self.inference:\n",
    "            col_idx = idx\n",
    "            col_name = self.column_names[col_idx]\n",
    "            col_data = self.normalized_values[:, col_idx]\n",
    "            file_id = self.file_ids[idx] if self.file_ids is not None else None\n",
    "            return {\n",
    "                \"column_name\": col_name,\n",
    "                \"input\": torch.from_numpy(col_data).unsqueeze(-1),  # (time_steps, 1)\n",
    "                \"file_id\": file_id\n",
    "            }\n",
    "\n",
    "        window_idx = idx // len(self.column_names)\n",
    "        col_idx = idx % len(self.column_names)\n",
    "\n",
    "        return {\n",
    "            \"column_name\": self.column_names[col_idx],\n",
    "            \"input\": torch.from_numpy(self.input_data[window_idx, :, col_idx]).unsqueeze(-1)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd43150-d37a-41b5-a870-317ae024e739",
   "metadata": {},
   "source": [
    "### a. 시계열 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "479a6dfb-a8b6-4cc8-bd02-7957f93f3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_A = TimeSeriesDataset(train_a, stride=60)\n",
    "train_dataset_B = TimeSeriesDataset(train_a, stride=60)\n",
    "\n",
    "train_dataset_A_B = torch.utils.data.ConcatDataset([train_dataset_A, train_dataset_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734db27-6d79-4231-8412-10c79c4d84d5",
   "metadata": {},
   "source": [
    "### b. data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24537d1e-37c0-4821-8d7b-2a635d596afe",
   "metadata": {},
   "source": [
    "`-` 모델 학습 과정 시 각 step 마다 데이터를 batch size 크기로 분할하여 넣어 효과적이고 효율적인 학습 진행을 돕기 위해 사용\n",
    "\n",
    "- 데이터를 1개씩 넣으면 학습이 너무 오래 걸리고, 전체를 한 번에 넣으면 컴퓨터 자우너이 데이터를 감당할 수 없기 때문에 위 같은 방법을 사용\n",
    "\n",
    "- DataLoader 객체는 일종의 generator 형태로, 인덱싱이 불가능하고 for문 순회 등의 방법을 통하여 분할된 데이터를 일일이 가져와야 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "cf7ab3f3-5182-4c00-a23f-95c605dccce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset_A_B, \n",
    "                                            batch_size=CFG.BATCH_SIZE, \n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a204ed-4a00-4300-9e4b-a81946163250",
   "metadata": {},
   "source": [
    "# AnomalyTransformer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "9d8b02f0-b0e2-4fa4-beb1-9adf9a05affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(AnomalyAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class AnomalyTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(AnomalyTransformerBlock, self).__init__()\n",
    "        self.attention = AnomalyAttention(d_model, n_heads)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention block with residual connection\n",
    "        x_residual = x\n",
    "        x = self.attention(x)\n",
    "        x = self.ln1(x + x_residual)\n",
    "\n",
    "        # Feedforward block with residual connection\n",
    "        x_residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.ln2(x + x_residual)\n",
    "        return x\n",
    "\n",
    "class AnomalyTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(AnomalyTransformer, self).__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Linear(1, cfg.HIDDEN_DIM_LSTM)\n",
    "\n",
    "        # Anomaly Transformer Blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            AnomalyTransformerBlock(d_model=cfg.HIDDEN_DIM_LSTM, n_heads=2)  # Reduced heads to 2\n",
    "            for _ in range(cfg.NUM_LAYERS)\n",
    "        ])\n",
    "\n",
    "        # Attention Score and Contribution Score Calculation Modules\n",
    "        self.attention_score_layer = nn.Linear(cfg.HIDDEN_DIM_LSTM, cfg.HIDDEN_DIM_LSTM)\n",
    "        self.contribution_score_layer = nn.Linear(cfg.HIDDEN_DIM_LSTM, cfg.HIDDEN_DIM_LSTM)\n",
    "\n",
    "        # Autoencoder Encoder modules\n",
    "        self.latent_encoder = nn.Sequential(\n",
    "            nn.Linear(cfg.HIDDEN_DIM_LSTM, cfg.HIDDEN_DIM_LSTM // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cfg.HIDDEN_DIM_LSTM // 4, cfg.HIDDEN_DIM_LSTM // 8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Autoencoder Decoder modules\n",
    "        self.latent_decoder = nn.Sequential(\n",
    "            nn.Linear(cfg.HIDDEN_DIM_LSTM // 8, cfg.HIDDEN_DIM_LSTM // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cfg.HIDDEN_DIM_LSTM // 4, cfg.HIDDEN_DIM_LSTM),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        # Pass through Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Attention Score and Contribution Score Calculation\n",
    "        attention_scores = torch.softmax(self.attention_score_layer(x), dim=-1)\n",
    "        contribution_scores = torch.softmax(self.contribution_score_layer(x), dim=-1)\n",
    "\n",
    "        # Calculate Divergence\n",
    "        divergence = F.kl_div(\n",
    "            input=torch.log(attention_scores + 1e-9),\n",
    "            target=contribution_scores,\n",
    "            reduction='batchmean'\n",
    "        )\n",
    "\n",
    "        # Autoencoder Operations\n",
    "        last_hidden = x[:, -1, :]  # Last time step hidden state\n",
    "        latent_z = self.latent_encoder(last_hidden)\n",
    "        reconstructed_hidden = self.latent_decoder(latent_z)\n",
    "\n",
    "        return last_hidden, reconstructed_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c254d737-a0f5-4211-b3e3-7971870ac896",
   "metadata": {},
   "source": [
    "# 학습함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "4d166877-a6db-4a92-abf3-6264c1f66540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AE(model, train_loader, optimizer, criterion, n_epochs, device):\n",
    "\n",
    "    train_losses = []\n",
    "    best_model = {\"loss\": float(\"inf\"), \"state\": None, \"epoch\": 0}\n",
    "\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()  # Mixed Precision Training\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\") as t:\n",
    "            for batch in t:\n",
    "                inputs = batch[\"input\"].to(device)\n",
    "\n",
    "                # Mixed Precision Forward Pass\n",
    "                with autocast():\n",
    "                    original_hidden, reconstructed_hidden = model(inputs)\n",
    "                    loss = criterion(reconstructed_hidden, original_hidden)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Mixed Precision Backward Pass\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                t.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Average Train Loss: {avg_epoch_loss:.8f}\")\n",
    "\n",
    "        if avg_epoch_loss < best_model[\"loss\"]:\n",
    "            best_model[\"state\"] = model.state_dict()\n",
    "            best_model[\"loss\"] = avg_epoch_loss\n",
    "            best_model[\"epoch\"] = epoch + 1\n",
    "\n",
    "        # Clear GPU memory cache to prevent OutOfMemory issues\n",
    "        import torch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return train_losses, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aefb8bb-c557-4e91-a8c8-c0a9e557af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "42436fe5-8a18-44f6-9986-3b823a1c86cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = AnomalyTransformer(CFG).to(rank)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(MODEL.parameters(), lr=CFG.LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca88f02-7a6d-425a-abe9-9bef07131c9d",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "3a4075d9-2504-4026-9762-ce4ddb1a9cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2764838/1393160410.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Mixed Precision Training\n",
      "Epoch 1/10:   0%|          | 0/884 [00:00<?, ?batch/s]/tmp/ipykernel_2764838/1393160410.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/10: 100%|██████████| 884/884 [00:29<00:00, 30.30batch/s, loss=9.39e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Train Loss: 0.01685050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 884/884 [00:29<00:00, 30.40batch/s, loss=5.74e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Average Train Loss: 0.00000195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 884/884 [00:28<00:00, 30.51batch/s, loss=8.75e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Average Train Loss: 0.00000547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 884/884 [00:28<00:00, 30.72batch/s, loss=3.24e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Average Train Loss: 0.00000716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 884/884 [00:28<00:00, 30.57batch/s, loss=4.22e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Average Train Loss: 0.00000756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 884/884 [00:29<00:00, 30.45batch/s, loss=3.83e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Average Train Loss: 0.00000785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 884/884 [00:28<00:00, 30.53batch/s, loss=3.73e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Average Train Loss: 0.00000881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 884/884 [00:28<00:00, 30.74batch/s, loss=1.02e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Average Train Loss: 0.00000818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 884/884 [00:28<00:00, 30.75batch/s, loss=6.64e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Average Train Loss: 0.00000876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 884/884 [00:28<00:00, 30.62batch/s, loss=6.78e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Average Train Loss: 0.00000727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, best_model = train_AE(\n",
    "    MODEL,\n",
    "    train_loader=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    n_epochs=CFG.EPOCHS,\n",
    "    device=rank,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "d52db12b-9c82-49ef-a9aa-04f96759f845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INFER_MODEL = AnomalyTransformer(CFG).to(rank)\n",
    "INFER_MODEL.load_state_dict(best_model[\"state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b1fd2-7bd1-42ab-8efe-0fee4c767e1f",
   "metadata": {},
   "source": [
    "# 임계치 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "c3c04f1e-1e38-49a7-a160-cef1120a7c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_save_threshold(MODEL, train_loader, percentile=98):\n",
    "    MODEL.eval()\n",
    "    train_errors = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs = batch[\"input\"].to(CFG.DEVICE)\n",
    "            original_hidden, reconstructed_hidden = MODEL(inputs)\n",
    "            mse_errors = torch.mean((original_hidden - reconstructed_hidden) ** 2, dim=1).cpu().numpy()\n",
    "            train_errors.extend(mse_errors)\n",
    "\n",
    "    threshold = np.percentile(train_errors, percentile)\n",
    "\n",
    "    print(f\"Threshold calculated and saved: {threshold}\")\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "1aa2419d-cd09-4c52-8c8d-78e3db6c0c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 884/884 [00:01<00:00, 552.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold calculated and saved: 5.7819752328214236e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = calculate_and_save_threshold(INFER_MODEL, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a449d4a5-8987-41bd-a6a4-4dd2a632f52c",
   "metadata": {},
   "source": [
    "# 추론 및 이상치 감지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "20421b17-8c14-47a1-8448-5e6bb65baadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    inputs = [item['input'] for item in batch]\n",
    "    file_ids = [item['file_id'] for item in batch]\n",
    "    column_names = [item['column_name'] for item in batch]\n",
    "    \n",
    "    # 패딩 추가 (최대 길이에 맞춤)\n",
    "    max_length = max(x.shape[0] for x in inputs)\n",
    "    padded_inputs = [torch.cat([x, torch.zeros(max_length - x.shape[0], x.shape[1])]) if x.shape[0] < max_length else x for x in inputs]\n",
    "    \n",
    "    inputs = torch.stack(padded_inputs)\n",
    "    return {\"input\": inputs, \"file_id\": file_ids, \"column_name\": column_names}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca03cff2-3a08-43ff-92c1-93d70cbf2198",
   "metadata": {},
   "source": [
    "## (1) 추론 및 test 데이터 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "4442ebce-29f0-45fc-ae94-b8d78d840c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test_files(MODEL, batch, device='cuda'):\n",
    "    MODEL.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = batch[\"input\"].to(device)\n",
    "        with autocast():  # Mixed Precision 활성화\n",
    "            original_hidden, reconstructed_hidden = MODEL(inputs)\n",
    "        reconstruction_loss = torch.mean((original_hidden - reconstructed_hidden) ** 2, dim=1).cpu().numpy()\n",
    "    return reconstruction_loss\n",
    "\n",
    "def test_data_processing(test_directory):\n",
    "    test_files = [f for f in os.listdir(test_directory) if f.startswith(\"TEST\") and f.endswith(\".csv\")]\n",
    "    test_datasets = []\n",
    "    all_test_data = []\n",
    "\n",
    "    for filename in tqdm(test_files, desc='Processing test files'):\n",
    "        test_file = os.path.join(test_directory, filename)\n",
    "        df = pd.read_csv(test_file)\n",
    "        df['file_id'] = filename.replace('.csv', '')\n",
    "        individual_df = df[['timestamp', 'file_id'] + df.filter(like='P').columns.tolist()]\n",
    "        individual_dataset = TimeSeriesDataset(individual_df, inference=True)\n",
    "        test_datasets.append(individual_dataset)\n",
    "        all_test_data.append(df)\n",
    "\n",
    "    combined_dataset = torch.utils.data.ConcatDataset(test_datasets)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        combined_dataset,\n",
    "        batch_size=CFG.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38befe7-d15f-4e43-acf9-65712259d38f",
   "metadata": {},
   "source": [
    "## (2) 이상치 추론함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "cc23a5e9-3f57-489e-b68a-e2caec579db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomaly(MODEL, test_loader):\n",
    "    reconstruction_errors = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        reconstruction_loss = inference_test_files(MODEL, batch, CFG.DEVICE)\n",
    "        for i in range(len(reconstruction_loss)):\n",
    "            reconstruction_errors.append({\n",
    "                \"ID\": batch[\"file_id\"][i],\n",
    "                \"column_name\": batch[\"column_name\"][i],\n",
    "                \"reconstruction_error\": reconstruction_loss[i]\n",
    "            })\n",
    "\n",
    "    errors_df = pd.DataFrame(reconstruction_errors)\n",
    "\n",
    "    flag_columns = []\n",
    "    for column in sorted(errors_df['column_name'].unique()):\n",
    "        flag_column = f'{column}_flag'\n",
    "        errors_df[flag_column] = (errors_df.loc[errors_df['column_name'] == column, 'reconstruction_error'] > THRESHOLD).astype(int)\n",
    "        flag_columns.append(flag_column)\n",
    "\n",
    "    errors_df_pivot = errors_df.pivot_table(index='ID', \n",
    "                                            columns='column_name', \n",
    "                                            values=flag_columns, \n",
    "                                            aggfunc='first')\n",
    "    errors_df_pivot.columns = [f'{col[1]}' for col in errors_df_pivot.columns]\n",
    "    errors_df_flat = errors_df_pivot.reset_index()\n",
    "\n",
    "    errors_df_flat['flag_list'] = errors_df_flat.loc[:, 'P1':'P' + str(len(flag_columns))].apply(lambda x: x.tolist(), axis=1).apply(lambda x: [int(i) for i in x])\n",
    "    return errors_df_flat[[\"ID\", \"flag_list\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "86e5f754-3764-420b-b92e-734a1a01dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test files: 100%|██████████| 2920/2920 [01:03<00:00, 46.20it/s]\n",
      "Processing test files: 100%|██████████| 2738/2738 [00:53<00:00, 51.20it/s]\n"
     ]
    }
   ],
   "source": [
    "C_DATA = test_data_processing(test_directory=\"./test/C\")\n",
    "D_DATA = test_data_processing(test_directory=\"./test/D\")\n",
    "\n",
    "#C_D_list = pd.concat([C_list, D_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "4f2cb744-a926-4729-84af-b3253389eac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fe3305d42e0>"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "1d873eb1-28b1-4afa-9872-aff975e00c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5840 [00:00<?, ?it/s]/tmp/ipykernel_2764838/3225321808.py:5: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed Precision 활성화\n",
      "  8%|▊         | 452/5840 [00:54<10:53,  8.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[295], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m C_list \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_anomaly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINFER_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_DATA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#D_list = detect_anomaly(INFER_MODEL, test_directory=\"./D\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[291], line 4\u001b[0m, in \u001b[0;36mdetect_anomaly\u001b[0;34m(MODEL, test_loader)\u001b[0m\n\u001b[1;32m      2\u001b[0m reconstruction_errors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader):\n\u001b[0;32m----> 4\u001b[0m     reconstruction_loss \u001b[38;5;241m=\u001b[39m \u001b[43minference_test_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(reconstruction_loss)):\n\u001b[1;32m      6\u001b[0m         reconstruction_errors\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_id\u001b[39m\u001b[38;5;124m\"\u001b[39m][i],\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_name\u001b[39m\u001b[38;5;124m\"\u001b[39m][i],\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreconstruction_error\u001b[39m\u001b[38;5;124m\"\u001b[39m: reconstruction_loss[i]\n\u001b[1;32m     10\u001b[0m         })\n",
      "Cell \u001b[0;32mIn[290], line 7\u001b[0m, in \u001b[0;36minference_test_files\u001b[0;34m(MODEL, batch, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():  \u001b[38;5;66;03m# Mixed Precision 활성화\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         original_hidden, reconstructed_hidden \u001b[38;5;241m=\u001b[39m MODEL(inputs)\n\u001b[0;32m----> 7\u001b[0m     reconstruction_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_hidden\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreconstructed_hidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reconstruction_loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "C_list = detect_anomaly(INFER_MODEL, C_DATA)\n",
    "#D_list = detect_anomaly(INFER_MODEL, test_directory=\"./D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a41195-414d-4ba6-b805-ed8723211cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "# 매핑된 값으로 업데이트하되, 매핑되지 않은 경우 기존 값 유지\n",
    "flag_mapping = C_D_list.set_index(\"ID\")[\"flag_list\"]\n",
    "sample_submission[\"flag_list\"] = sample_submission[\"ID\"].map(flag_mapping).fillna(sample_submission[\"flag_list\"])\n",
    "\n",
    "sample_submission.to_csv(\"./baseline_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592c731f-210a-49c4-bdc6-74aab783e8dc",
   "metadata": {},
   "source": [
    "# do next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c4c3f-c607-4c0a-86cb-f813855197e2",
   "metadata": {},
   "source": [
    "`1` github 코드 전부 반영\n",
    "\n",
    "`2` val_data set 나누어서 평가\n",
    "\n",
    "`3` 모델 성능 확인해서 리더보드 업로드"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
